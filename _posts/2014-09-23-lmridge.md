---
layout: post
title: "RのMASSパッケージのlm.ridge関数のソースコードを読む"
description: ""
category: ソースコードを読む
tags: [ソースコードを読む,R]
---
{% include JB/setup %}

RのMASSパッケージのlm.ridge関数のソースコードを見たのだが，ひと目では何をしているのかさっぱりわからなかった．

ので，解読した結果をメモしておく．

### リッジ回帰
$\mathrm{X} \in \mathbb{R}^{n \times p}$を説明変数を集めた行列，$\boldsymbol{y} \in \mathbb{R}^n$ を従属変数とする．プログラム中では $\mathrm{X}$ は `X` に，$\boldsymbol{y}$ は `Y` に対応する．このとき，リッジ回帰による回帰係数は以下のように書ける．

$$
\begin{align}
	\hat{\boldsymbol{\beta}}_{\mathrm{ridge}} = \mathop{\mathrm{argmin}}_{\boldsymbol{\beta}\in\mathbb{R}^p}
	\left\{\|\boldsymbol{y} - \mathrm{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|^2\right\}
\end{align}
$$

以下のように記号を定義する．

$$
\begin{align}
	\boldsymbol{y}^m &:= \frac{1}{n}\sum_{i=1}^n y_i \\
	\mathrm{X}^m &:= \left[ \mathrm{X}_{\cdot,1} \cdots \mathrm{X}_{\cdot,p} \right] \in \mathbb{R}^{1 \times p} \\
	\mathrm{X}_{\cdot,j} &:= \frac{1}{n}\sum_{i=1}^n \mathrm{X}_{i,j} \quad (j=1,\ldots,p)\\
	s_j &:= \frac{1}{n}\sum_{i=1}^n \mathrm{X}_{i,j}^2 \quad (j=1,\ldots,p)
\end{align}
$$

$\mathrm{X}^m, \boldsymbol{y}^m$はそれぞれ`Xm`,`Ym`に相当する．

次に $\mathrm{X}$ をスケーリングした新しいデータ行列 $\tilde{\mathrm{X}}$ を以下のように定義する．

$$
	\left( \tilde{\mathrm{X}} \right)_{i,j} = \frac{\mathrm{X}_{i,j} - \mathrm{X}_{\cdot,j}}{\sqrt{s_j}}
		\quad (i=1, \ldots, n,\, j=1, \ldots, p)
$$


プログラム中の `X <- X/rep(Xscale, rep(n, p))` の時点での `X` が $\tilde{\mathrm{X}}$ に相当する．

$\tilde{\mathrm{X}}$ のSVDを $\tilde{\mathrm{X}}=\mathrm{UDV}^\top$と定義する．$\mathrm{U,D,V}$はそれぞれ，$n \times p$, $p \times p$, $p \times p$行列であり，以下のように書けるとする．

$$
\begin{align}
	\mathrm{U} = \left( \boldsymbol{u}_1 \cdots \boldsymbol{u}_p \right), \quad
	\mathrm{D} = \mathop{\mathrm{diag}}(d_1,\ldots,d_p)
%	\begin{pmatrix}
%		d_1 &  & \\
%		 & \ddots & \\
%		& & d_p
%		\end{pmatrix}
		, \quad
	\mathrm{V} = \left( \boldsymbol{v}_1 \cdots \boldsymbol{v}_p \right)
\end{align}
$$

このときリッジ回帰の解は以下のように書ける．

$$
\begin{align}
	\hat{\boldsymbol{\beta}}_{\mathrm{ridge}}
	&= \sum_{i=1}^p \frac{1}{ {d_i}^2 + \lambda } \boldsymbol{v}_i {\boldsymbol{v}_i}^T \tilde{\mathrm{X}}^T \boldsymbol{y}
	= \mathrm{V \Lambda D}^\top \mathrm{U}^\top \boldsymbol{y} \\
	\mathrm{\Lambda} &:= \mathop{\mathrm{diag}}\left( \frac{1}{ {d_1}^2 + \lambda }, \cdots, \frac{1}{ {d_p}^2 + \lambda } \right)
\end{align}
$$

$\hat{\boldsymbol{\beta}}_{\mathrm{ridge}}$がソースコード中の`coef`に相当する．


### `lm.ridge`のソースコード
MASSパッケージ中のlm.ridge関数のソースコードを以下に載せておく．(バージョンは7.3-29)

{% highlight R %}
> lm.ridge
function (formula, data, subset, na.action, lambda = 0, model = FALSE, 
    x = FALSE, y = FALSE, contrasts = NULL, ...) 
{
    m <- match.call(expand.dots = FALSE)
    m$model <- m$x <- m$y <- m$contrasts <- m$... <- m$lambda <- NULL
    m[[1L]] <- quote(stats::model.frame)
    m <- eval.parent(m)
    Terms <- attr(m, "terms")
    Y <- model.response(m)
    X <- model.matrix(Terms, m, contrasts)
    n <- nrow(X)
    p <- ncol(X)
    offset <- model.offset(m)
    if (!is.null(offset)) 
        Y <- Y - offset
    if (Inter <- attr(Terms, "intercept")) {
        Xm <- colMeans(X[, -Inter])
        Ym <- mean(Y)
        p <- p - 1
        X <- X[, -Inter] - rep(Xm, rep(n, p))
        Y <- Y - Ym
    }
    else Ym <- Xm <- NA
    Xscale <- drop(rep(1/n, n) %*% X^2)^0.5
    X <- X/rep(Xscale, rep(n, p))
    Xs <- svd(X)
    rhs <- t(Xs$u) %*% Y
    d <- Xs$d
    lscoef <- Xs$v %*% (rhs/d)
    lsfit <- X %*% lscoef
    resid <- Y - lsfit
    s2 <- sum(resid^2)/(n - p - Inter)
    HKB <- (p - 2) * s2/sum(lscoef^2)
    LW <- (p - 2) * s2 * n/sum(lsfit^2)
    k <- length(lambda)
    dx <- length(d)
    div <- d^2 + rep(lambda, rep(dx, k))
    a <- drop(d * rhs)/div
    dim(a) <- c(dx, k)
    coef <- Xs$v %*% a
    dimnames(coef) <- list(names(Xscale), format(lambda))
    GCV <- colSums((Y - X %*% coef)^2)/(n - colSums(matrix(d^2/div, 
        dx)))^2
    res <- list(coef = drop(coef), scales = Xscale, Inter = Inter, 
        lambda = lambda, ym = Ym, xm = Xm, GCV = GCV, kHKB = HKB, 
        kLW = LW)
    class(res) <- "ridgelm"
    res
}
<bytecode: 0x450f980>
<environment: namespace:MASS>
{% endhighlight %}


### その他
`kHKB`と`kLW`はリッジ回帰のチューニングパラメータ $\lambda$ の候補である．詳しくは3つ目の参考文献の論文(とその参考文献)に書いてある．

### 参考文献

- Hoerl, A., & Kennard, R. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55–67. Retrieved from <http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634>
- [cran.r-project.org/web/packages/MASS/MASS.pdf](http://cran.r-project.org/web/packages/MASS/MASS.pdf)
- Dorugade, A. V, & Kashid, D. N. (2010). Alternative Method for Choosing Ridge Parameter for Regression, 4(9), 447–456. ([pdf file](http://www.m-hikari.com/ams/ams-2010/ams-9-12-2010/dorugadeAMS9-12-2010.pdf))
